\section{Discussion}
\label{sec:discussion}

\begin{itemize}
\item MPI
\item Interpretation of result with graph is inverted resp. not inverted
\item METIS weights, dynamic partitioning
\item Slowdown for many threads on tussilago and vitsippa
\item Tuning parameters for better performance plus strange phenomena when too high prod. rate
\item Comment on speedup and sizeup
\end{itemize}

Judging by the results presented in Figure \ref{fig:speedup_sizeup} the parallelization gave significant speedup and sizeup. The resulting speedup is roughly $0.5$ times the number of cores, which is a good speedup. The sizeup is also good at almost $0.5$ times number of cores. The drop in speedup at $16$ cores can be due to that the servers \textit{vitsippa.it.uu.se} and \textit{tussilago.it.uu.se} are dual socket and therefore have non uniform memory access times. Since the servers each have $32$ cores using $16$ of them might result in conflicts with other processes running on the servers. If another process runs for just a moment one of the $16$ cores might have to wait. This increases the execution time since all cores need to synchronize. The probability for this phenomena to happen increases with the number of cores being used.

The reason for the sizeup being less than the speed up is due to cache misses. Since the cache relatively decreases when the problem size is increased there will be more cache misses for bigger problems, hence the sizeup will not be ass great as the speedup. 

To improve performance of the implementation, it may be useful to weight each node in the graph partitioning according to the number of neighbors that each node has, since the computational demand scales with the number of neighbors. It may also help to dynamically re-partition the graph, taking advantage of the fact that no computations need to be performed at nodes that do not contain any particles. These changes would increase the difference in number of nodes assigned to each core, which would be problematic, considering that the current implementation assigns an equal number of nodes to each core and there may not be a simple way to assign a different number of nodes to each core using OpenMP. This problem could be solved by utilizing POSIX Threads (Pthreads) instead of OpenMP. By using Pthreads the programmer is given more control over the specific work that is performed by each core. In this way nodes could be transferred between cores dynamically while still using a shared memory model.

A natural progression of this work is to introduce communication between cores that do not share memory, e.g. via message passing. This would likely only provide performance improvements for large problems, since it relies on higher-latency communication.